[2025-06-02T16:29:19.136+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-06-02T16:29:19.159+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T15:37:47.618112+00:00 [queued]>
[2025-06-02T16:29:19.171+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T15:37:47.618112+00:00 [queued]>
[2025-06-02T16:29:19.172+0000] {taskinstance.py:2865} INFO - Starting attempt 4 of 4
[2025-06-02T16:29:19.226+0000] {taskinstance.py:2888} INFO - Executing <Task(BashOperator): start_spark_consumer> on 2025-06-02 15:37:47.618112+00:00
[2025-06-02T16:29:19.235+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'log_monitoring_dag', 'start_spark_consumer', 'manual__2025-06-02T15:37:47.618112+00:00', '--job-id', '126', '--raw', '--subdir', 'DAGS_FOLDER/log_monitoring_dag.py', '--cfg-path', '/tmp/tmpq1y9si07']
[2025-06-02T16:29:19.238+0000] {standard_task_runner.py:105} INFO - Job 126: Subtask start_spark_consumer
[2025-06-02T16:29:19.298+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=2011) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-06-02T16:29:19.301+0000] {standard_task_runner.py:72} INFO - Started process 2012 to run task
[2025-06-02T16:29:19.309+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:209 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-06-02T16:29:19.521+0000] {task_command.py:467} INFO - Running <TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T15:37:47.618112+00:00 [running]> on host c47ce43ea693
[2025-06-02T16:29:20.056+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='log_monitoring_dag' AIRFLOW_CTX_TASK_ID='start_spark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-06-02T15:37:47.618112+00:00' AIRFLOW_CTX_TRY_NUMBER='4' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-02T15:37:47.618112+00:00'
[2025-06-02T16:29:20.057+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-06-02T16:29:20.204+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-06-02T16:29:20.205+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'bash -c "echo Starting Spark Consumer && docker exec realtime_log_monitoring-spark-1 spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /app/log_consumer.py && echo Finished Spark Consumer"']
[2025-06-02T16:29:20.218+0000] {subprocess.py:86} INFO - Output:
[2025-06-02T16:29:20.230+0000] {subprocess.py:93} INFO - Starting Spark Consumer
[2025-06-02T16:29:24.440+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-06-02T16:29:24.541+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-06-02T16:29:24.542+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-06-02T16:29:24.545+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-06-02T16:29:24.546+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-9c9a300f-de9c-4ab2-a263-0fabc8fcae3b;1.0
[2025-06-02T16:29:24.547+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-06-02T16:29:24.692+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2025-06-02T16:29:24.804+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2025-06-02T16:29:24.985+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;2.8.1 in central
[2025-06-02T16:29:25.089+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-06-02T16:29:25.299+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2025-06-02T16:29:25.399+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;1.7.32 in central
[2025-06-02T16:29:25.492+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2025-06-02T16:29:25.577+0000] {subprocess.py:93} INFO - 	found org.spark-project.spark#unused;1.0.0 in central
[2025-06-02T16:29:25.673+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2025-06-02T16:29:25.700+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-06-02T16:29:25.788+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-06-02T16:29:25.884+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-06-02T16:29:25.981+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1353ms :: artifacts dl 82ms
[2025-06-02T16:29:25.982+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-06-02T16:29:25.983+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-06-02T16:29:25.983+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-06-02T16:29:25.984+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-06-02T16:29:25.985+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
[2025-06-02T16:29:25.985+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
[2025-06-02T16:29:25.986+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
[2025-06-02T16:29:25.986+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
[2025-06-02T16:29:25.987+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
[2025-06-02T16:29:25.987+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-06-02T16:29:25.987+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.32 from central in [default]
[2025-06-02T16:29:25.988+0000] {subprocess.py:93} INFO - 	org.spark-project.spark#unused;1.0.0 from central in [default]
[2025-06-02T16:29:25.988+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
[2025-06-02T16:29:25.989+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T16:29:25.990+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-06-02T16:29:25.990+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-06-02T16:29:25.991+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T16:29:25.991+0000] {subprocess.py:93} INFO - 	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
[2025-06-02T16:29:25.992+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T16:29:25.992+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-9c9a300f-de9c-4ab2-a263-0fabc8fcae3b
[2025-06-02T16:29:25.992+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-06-02T16:29:25.997+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 12 already retrieved (0kB/8ms)
[2025-06-02T16:29:26.710+0000] {subprocess.py:93} INFO - 25/06/02 16:29:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-02T16:29:28.856+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO SparkContext: Running Spark version 3.3.0
[2025-06-02T16:29:28.884+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceUtils: ==============================================================
[2025-06-02T16:29:28.886+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-02T16:29:28.890+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceUtils: ==============================================================
[2025-06-02T16:29:28.893+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO SparkContext: Submitted application: KafkaLogConsumerBatch
[2025-06-02T16:29:28.921+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-02T16:29:28.933+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceProfile: Limiting resource is cpu
[2025-06-02T16:29:28.934+0000] {subprocess.py:93} INFO - 25/06/02 16:29:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-02T16:29:29.029+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SecurityManager: Changing view acls to: spark
[2025-06-02T16:29:29.030+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SecurityManager: Changing modify acls to: spark
[2025-06-02T16:29:29.031+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SecurityManager: Changing view acls groups to:
[2025-06-02T16:29:29.031+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SecurityManager: Changing modify acls groups to:
[2025-06-02T16:29:29.032+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
[2025-06-02T16:29:29.621+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO Utils: Successfully started service 'sparkDriver' on port 36743.
[2025-06-02T16:29:29.708+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SparkEnv: Registering MapOutputTracker
[2025-06-02T16:29:29.748+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-02T16:29:29.788+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-02T16:29:29.788+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-02T16:29:29.793+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-02T16:29:29.812+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-803eba6a-40cf-47b9-a7dc-b5aab459c5cd
[2025-06-02T16:29:29.827+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2025-06-02T16:29:29.842+0000] {subprocess.py:93} INFO - 25/06/02 16:29:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-02T16:29:30.111+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-02T16:29:30.144+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://41026342dcb2:36743/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.145+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://41026342dcb2:36743/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.146+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://41026342dcb2:36743/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748881768847
[2025-06-02T16:29:30.146+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://41026342dcb2:36743/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.147+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://41026342dcb2:36743/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748881768847
[2025-06-02T16:29:30.147+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://41026342dcb2:36743/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.148+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:30.149+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://41026342dcb2:36743/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.149+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://41026342dcb2:36743/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748881768847
[2025-06-02T16:29:30.150+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://41026342dcb2:36743/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748881768847
[2025-06-02T16:29:30.150+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:30.151+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://41026342dcb2:36743/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748881768847
[2025-06-02T16:29:30.152+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.154+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:30.178+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.179+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:30.332+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748881768847
[2025-06-02T16:29:30.333+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T16:29:30.714+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.714+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T16:29:30.817+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748881768847
[2025-06-02T16:29:30.818+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T16:29:30.883+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:30.983+0000] {subprocess.py:93} INFO - 25/06/02 16:29:30 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T16:29:31.057+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:31.058+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T16:29:31.758+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748881768847
[2025-06-02T16:29:31.758+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T16:29:31.979+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748881768847
[2025-06-02T16:29:31.980+0000] {subprocess.py:93} INFO - 25/06/02 16:29:31 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T16:29:32.092+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748881768847
[2025-06-02T16:29:32.092+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T16:29:32.448+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:32.449+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T16:29:32.825+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748881768847
[2025-06-02T16:29:32.825+0000] {subprocess.py:93} INFO - 25/06/02 16:29:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T16:29:33.099+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Starting executor ID driver on host 41026342dcb2
[2025-06-02T16:29:33.106+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-06-02T16:29:33.117+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748881768847
[2025-06-02T16:29:33.136+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T16:29:33.239+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:33.263+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T16:29:33.456+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748881768847
[2025-06-02T16:29:33.457+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T16:29:33.633+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748881768847
[2025-06-02T16:29:33.634+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T16:29:33.832+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748881768847
[2025-06-02T16:29:33.833+0000] {subprocess.py:93} INFO - 25/06/02 16:29:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T16:29:34.034+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748881768847
[2025-06-02T16:29:34.039+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T16:29:34.099+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:34.100+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:34.214+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:34.215+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T16:29:34.771+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748881768847
[2025-06-02T16:29:34.773+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T16:29:34.914+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:34.929+0000] {subprocess.py:93} INFO - 25/06/02 16:29:34 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T16:29:35.274+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:35.274+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:35.332+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:35.333+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T16:29:35.636+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748881768847
[2025-06-02T16:29:35.681+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO TransportClientFactory: Successfully created connection to 41026342dcb2/172.18.0.7:36743 after 32 ms (0 ms spent in bootstraps)
[2025-06-02T16:29:35.688+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp5183162656217888796.tmp
[2025-06-02T16:29:35.813+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp5183162656217888796.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T16:29:35.833+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader
[2025-06-02T16:29:35.834+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:35.834+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp7598166117110977585.tmp
[2025-06-02T16:29:35.836+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp7598166117110977585.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:35.864+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader
[2025-06-02T16:29:35.865+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748881768847
[2025-06-02T16:29:35.865+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp1409922554663420574.tmp
[2025-06-02T16:29:35.867+0000] {subprocess.py:93} INFO - 25/06/02 16:29:35 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp1409922554663420574.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T16:29:36.048+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.commons_commons-pool2-2.11.1.jar to class loader
[2025-06-02T16:29:36.048+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Fetching spark://41026342dcb2:36743/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748881768847
[2025-06-02T16:29:36.049+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: Fetching spark://41026342dcb2:36743/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp8142358512504545978.tmp
[2025-06-02T16:29:36.051+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp8142358512504545978.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T16:29:36.091+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/commons-logging_commons-logging-1.1.3.jar to class loader
[2025-06-02T16:29:36.092+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:36.093+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp3033723517645106438.tmp
[2025-06-02T16:29:36.757+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp3033723517645106438.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T16:29:36.791+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader
[2025-06-02T16:29:36.791+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748881768847
[2025-06-02T16:29:36.792+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp1606298829286786620.tmp
[2025-06-02T16:29:36.841+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp1606298829286786620.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T16:29:36.965+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.kafka_kafka-clients-2.8.1.jar to class loader
[2025-06-02T16:29:36.966+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748881768847
[2025-06-02T16:29:36.966+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp7065836271766857945.tmp
[2025-06-02T16:29:36.968+0000] {subprocess.py:93} INFO - 25/06/02 16:29:36 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp7065836271766857945.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T16:29:37.300+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.slf4j_slf4j-api-1.7.32.jar to class loader
[2025-06-02T16:29:37.300+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748881768847
[2025-06-02T16:29:37.301+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp805771356140051014.tmp
[2025-06-02T16:29:37.318+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp805771356140051014.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T16:29:37.325+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader
[2025-06-02T16:29:37.326+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748881768847
[2025-06-02T16:29:37.326+0000] {subprocess.py:93} INFO - 25/06/02 16:29:37 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp3583080633664205509.tmp
[2025-06-02T16:29:38.518+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp3583080633664205509.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T16:29:38.625+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader
[2025-06-02T16:29:38.626+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Fetching spark://41026342dcb2:36743/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:38.626+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: Fetching spark://41026342dcb2:36743/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp4553598948358544035.tmp
[2025-06-02T16:29:38.627+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp4553598948358544035.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T16:29:38.770+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/com.google.code.findbugs_jsr305-3.0.0.jar to class loader
[2025-06-02T16:29:38.771+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748881768847
[2025-06-02T16:29:38.772+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp8752955489357566027.tmp
[2025-06-02T16:29:38.786+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp8752955489357566027.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T16:29:38.820+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.spark-project.spark_unused-1.0.0.jar to class loader
[2025-06-02T16:29:38.825+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Executor: Fetching spark://41026342dcb2:36743/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748881768847
[2025-06-02T16:29:38.826+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: Fetching spark://41026342dcb2:36743/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp4790637750724419226.tmp
[2025-06-02T16:29:38.906+0000] {subprocess.py:93} INFO - 25/06/02 16:29:38 INFO Utils: /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/fetchFileTemp4790637750724419226.tmp has been previously copied to /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T16:29:39.160+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO Executor: Adding file:/tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/userFiles-87486b10-2f03-4f61-bbc5-38afa6a9d26b/org.lz4_lz4-java-1.8.0.jar to class loader
[2025-06-02T16:29:39.168+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46639.
[2025-06-02T16:29:39.169+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO NettyBlockTransferService: Server created on 41026342dcb2:46639
[2025-06-02T16:29:39.171+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-02T16:29:39.179+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 41026342dcb2, 46639, None)
[2025-06-02T16:29:39.184+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO BlockManagerMasterEndpoint: Registering block manager 41026342dcb2:46639 with 366.3 MiB RAM, BlockManagerId(driver, 41026342dcb2, 46639, None)
[2025-06-02T16:29:39.186+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 41026342dcb2, 46639, None)
[2025-06-02T16:29:39.188+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 41026342dcb2, 46639, None)
[2025-06-02T16:29:39.852+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-02T16:29:39.854+0000] {subprocess.py:93} INFO - 25/06/02 16:29:39 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-06-02T16:29:41.527+0000] {subprocess.py:93} INFO - 25/06/02 16:29:41 INFO CodeGenerator: Code generated in 326.327139 ms
[2025-06-02T16:29:41.585+0000] {subprocess.py:93} INFO - 25/06/02 16:29:41 INFO CodeGenerator: Code generated in 47.137563 ms
[2025-06-02T16:29:46.608+0000] {subprocess.py:93} INFO - 25/06/02 16:29:46 INFO ConsumerConfig: ConsumerConfig values:
[2025-06-02T16:29:46.608+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-06-02T16:29:46.609+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-06-02T16:29:46.609+0000] {subprocess.py:93} INFO - 	auto.offset.reset = earliest
[2025-06-02T16:29:46.610+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-06-02T16:29:46.610+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-06-02T16:29:46.610+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-06-02T16:29:46.611+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1
[2025-06-02T16:29:46.611+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-06-02T16:29:46.611+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-06-02T16:29:46.612+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-06-02T16:29:46.612+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-06-02T16:29:46.612+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-06-02T16:29:46.613+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-06-02T16:29:46.613+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-06-02T16:29:46.613+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-06-02T16:29:46.614+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0
[2025-06-02T16:29:46.614+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-06-02T16:29:46.614+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-06-02T16:29:46.614+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-06-02T16:29:46.615+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-06-02T16:29:46.615+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-06-02T16:29:46.615+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-06-02T16:29:46.616+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T16:29:46.616+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-06-02T16:29:46.616+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-06-02T16:29:46.617+0000] {subprocess.py:93} INFO - 	max.poll.records = 1
[2025-06-02T16:29:46.617+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-06-02T16:29:46.617+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-06-02T16:29:46.618+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-06-02T16:29:46.618+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-06-02T16:29:46.618+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-06-02T16:29:46.619+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[2025-06-02T16:29:46.619+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-06-02T16:29:46.619+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-06-02T16:29:46.620+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-06-02T16:29:46.620+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-06-02T16:29:46.620+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-06-02T16:29:46.621+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-06-02T16:29:46.621+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-06-02T16:29:46.621+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-06-02T16:29:46.621+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-06-02T16:29:46.622+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-06-02T16:29:46.622+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-06-02T16:29:46.622+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-06-02T16:29:46.623+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-06-02T16:29:46.623+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-06-02T16:29:46.623+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-06-02T16:29:46.624+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-06-02T16:29:46.624+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-06-02T16:29:46.625+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-06-02T16:29:46.625+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-06-02T16:29:46.625+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-06-02T16:29:46.626+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-06-02T16:29:46.626+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-06-02T16:29:46.626+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 10000
[2025-06-02T16:29:46.627+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-06-02T16:29:46.627+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-06-02T16:29:46.628+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-06-02T16:29:46.689+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2]
[2025-06-02T16:29:46.690+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-06-02T16:29:46.690+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-06-02T16:29:46.690+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-06-02T16:29:46.691+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-06-02T16:29:46.691+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-06-02T16:29:46.692+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-06-02T16:29:46.692+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-06-02T16:29:46.692+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-06-02T16:29:46.693+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-06-02T16:29:46.693+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.2
[2025-06-02T16:29:46.693+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-06-02T16:29:46.694+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-06-02T16:29:46.694+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-06-02T16:29:46.694+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-06-02T16:29:46.695+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-06-02T16:29:46.695+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-06-02T16:29:46.695+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-06-02T16:29:46.696+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T16:29:46.696+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:29:46.778+0000] {subprocess.py:93} INFO - 25/06/02 16:29:46 INFO AppInfoParser: Kafka version: 2.8.1
[2025-06-02T16:29:46.779+0000] {subprocess.py:93} INFO - 25/06/02 16:29:46 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2025-06-02T16:29:46.779+0000] {subprocess.py:93} INFO - 25/06/02 16:29:46 INFO AppInfoParser: Kafka startTimeMs: 1748881786772
[2025-06-02T16:29:46.782+0000] {subprocess.py:93} INFO - 25/06/02 16:29:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Subscribed to topic(s): logs
[2025-06-02T16:29:47.124+0000] {subprocess.py:93} INFO - 25/06/02 16:29:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Cluster ID: KseVdgT7Th2WPRTGnxOBbw
[2025-06-02T16:29:47.172+0000] {subprocess.py:93} INFO - 25/06/02 16:29:47 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
[2025-06-02T16:29:58.333+0000] {job.py:229} INFO - Heartbeat recovered after 13.48 seconds
[2025-06-02T16:30:26.264+0000] {job.py:229} INFO - Heartbeat recovered after 19.22 seconds
[2025-06-02T16:30:42.480+0000] {subprocess.py:93} INFO - 25/06/02 16:30:42 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] (Re-)joining group
[2025-06-02T16:30:45.129+0000] {subprocess.py:93} INFO - 25/06/02 16:30:45 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] (Re-)joining group
[2025-06-02T16:30:48.236+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1-229aeb2b-780b-4e99-950d-2e6ef91797a9', protocol='range'}
[2025-06-02T16:30:48.293+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1-229aeb2b-780b-4e99-950d-2e6ef91797a9=Assignment(partitions=[logs-0])}
[2025-06-02T16:30:48.500+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1-229aeb2b-780b-4e99-950d-2e6ef91797a9', protocol='range'}
[2025-06-02T16:30:48.501+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Notifying assignor about the new Assignment(partitions=[logs-0])
[2025-06-02T16:30:48.505+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Adding newly assigned partitions: logs-0
[2025-06-02T16:30:48.513+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Found no committed offset for partition logs-0
[2025-06-02T16:30:48.909+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Revoke previously assigned partitions logs-0
[2025-06-02T16:30:48.910+0000] {subprocess.py:93} INFO - 25/06/02 16:30:48 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0] Member consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1-229aeb2b-780b-4e99-950d-2e6ef91797a9 sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
[2025-06-02T16:30:49.068+0000] {subprocess.py:93} INFO - 25/06/02 16:30:49 INFO Metrics: Metrics scheduler closed
[2025-06-02T16:30:49.069+0000] {subprocess.py:93} INFO - 25/06/02 16:30:49 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-06-02T16:30:49.069+0000] {subprocess.py:93} INFO - 25/06/02 16:30:49 INFO Metrics: Metrics reporters closed
[2025-06-02T16:30:49.604+0000] {subprocess.py:93} INFO - 25/06/02 16:30:49 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-driver-0-1 unregistered
[2025-06-02T16:30:49.605+0000] {subprocess.py:93} INFO - 25/06/02 16:30:49 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(logs-0,-2,-1,None)
[2025-06-02T16:30:55.363+0000] {subprocess.py:93} INFO - 25/06/02 16:30:55 INFO CodeGenerator: Code generated in 90.751968 ms
[2025-06-02T16:31:05.882+0000] {subprocess.py:93} INFO - 25/06/02 16:31:05 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,json at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:775)
[2025-06-02T16:31:05.883+0000] {subprocess.py:93} INFO - sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-06-02T16:31:05.883+0000] {subprocess.py:93} INFO - sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-06-02T16:31:05.884+0000] {subprocess.py:93} INFO - sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-06-02T16:31:05.884+0000] {subprocess.py:93} INFO - java.lang.reflect.Method.invoke(Method.java:498)
[2025-06-02T16:31:05.885+0000] {subprocess.py:93} INFO - py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-06-02T16:31:05.885+0000] {subprocess.py:93} INFO - py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2025-06-02T16:31:05.885+0000] {subprocess.py:93} INFO - py4j.Gateway.invoke(Gateway.java:282)
[2025-06-02T16:31:05.886+0000] {subprocess.py:93} INFO - py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-06-02T16:31:05.886+0000] {subprocess.py:93} INFO - py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-06-02T16:31:05.887+0000] {subprocess.py:93} INFO - py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-06-02T16:31:05.887+0000] {subprocess.py:93} INFO - py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-06-02T16:31:05.887+0000] {subprocess.py:93} INFO - java.lang.Thread.run(Thread.java:750),== Physical Plan ==
[2025-06-02T16:31:05.888+0000] {subprocess.py:93} INFO - Execute InsertIntoHadoopFsRelationCommand (5)
[2025-06-02T16:31:05.888+0000] {subprocess.py:93} INFO - +- * Project (4)
[2025-06-02T16:31:05.889+0000] {subprocess.py:93} INFO -    +- Project (3)
[2025-06-02T16:31:05.889+0000] {subprocess.py:93} INFO -       +- Filter (2)
[2025-06-02T16:31:05.889+0000] {subprocess.py:93} INFO -          +- * Scan KafkaRelation(strategy=Subscribe[logs], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)  (1)
[2025-06-02T16:31:05.890+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.890+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.890+0000] {subprocess.py:93} INFO - (1) Scan KafkaRelation(strategy=Subscribe[logs], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)  [codegen id : 1]
[2025-06-02T16:31:05.891+0000] {subprocess.py:93} INFO - Output [7]: [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
[2025-06-02T16:31:05.891+0000] {subprocess.py:93} INFO - ReadSchema: struct<key:binary,value:binary,topic:string,partition:int,offset:bigint,timestamp:timestamp,timestampType:int>
[2025-06-02T16:31:05.891+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.892+0000] {subprocess.py:93} INFO - (2) Filter
[2025-06-02T16:31:05.892+0000] {subprocess.py:93} INFO - Input [7]: [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
[2025-06-02T16:31:05.892+0000] {subprocess.py:93} INFO - Condition : (isnotnull(value#1) AND (from_json(StructField(level,StringType,true), cast(value#1 as string), Some(Etc/UTC)).level = ERROR))
[2025-06-02T16:31:05.892+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.893+0000] {subprocess.py:93} INFO - (3) Project
[2025-06-02T16:31:05.893+0000] {subprocess.py:93} INFO - Output [1]: [from_json(StructField(timestamp,StringType,true), StructField(level,StringType,true), StructField(message,StringType,true), cast(value#1 as string), Some(Etc/UTC)) AS data#16]
[2025-06-02T16:31:05.893+0000] {subprocess.py:93} INFO - Input [7]: [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
[2025-06-02T16:31:05.894+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.894+0000] {subprocess.py:93} INFO - (4) Project [codegen id : 2]
[2025-06-02T16:31:05.894+0000] {subprocess.py:93} INFO - Output [3]: [data#16.timestamp AS timestamp#18, data#16.level AS level#19, data#16.message AS message#20]
[2025-06-02T16:31:05.894+0000] {subprocess.py:93} INFO - Input [1]: [data#16]
[2025-06-02T16:31:05.895+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.895+0000] {subprocess.py:93} INFO - (5) Execute InsertIntoHadoopFsRelationCommand
[2025-06-02T16:31:05.895+0000] {subprocess.py:93} INFO - Input [3]: [timestamp#18, level#19, message#20]
[2025-06-02T16:31:05.896+0000] {subprocess.py:93} INFO - Arguments: file:/app/output/error_logs, false, JSON, [path=/app/output/error_logs], Append, [timestamp, level, message]
[2025-06-02T16:31:05.896+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:05.896+0000] {subprocess.py:93} INFO - ,org.apache.spark.sql.execution.SparkPlanInfo@b6408c65,1748881863611,Map()) by listener SQLAppStatusListener took 1.843855732s.
[2025-06-02T16:31:07.601+0000] {subprocess.py:93} INFO - 25/06/02 16:31:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-02T16:31:07.602+0000] {subprocess.py:93} INFO - 25/06/02 16:31:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-02T16:31:07.603+0000] {subprocess.py:93} INFO - 25/06/02 16:31:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2025-06-02T16:31:09.900+0000] {subprocess.py:93} INFO - 25/06/02 16:31:09 INFO CodeGenerator: Code generated in 15.227268 ms
[2025-06-02T16:31:09.974+0000] {subprocess.py:93} INFO - 25/06/02 16:31:09 INFO CodeGenerator: Code generated in 74.679217 ms
[2025-06-02T16:31:10.103+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2025-06-02T16:31:10.534+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-02T16:31:10.632+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2025-06-02T16:31:10.633+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO DAGScheduler: Parents of final stage: List()
[2025-06-02T16:31:10.633+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO DAGScheduler: Missing parents: List()
[2025-06-02T16:31:10.634+0000] {subprocess.py:93} INFO - 25/06/02 16:31:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-02T16:31:10.638+0000] {job.py:229} INFO - Heartbeat recovered after 12.41 seconds
[2025-06-02T16:31:11.759+0000] {subprocess.py:93} INFO - 25/06/02 16:31:11 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1748881870535,WrappedArray(org.apache.spark.scheduler.StageInfo@12113190),{spark.sql.warehouse.dir=file:/app/spark-warehouse, spark.app.initial.file.urls=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.app.initial.jar.urls=spark://41026342dcb2:36743/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,spark://41026342dcb2:36743/jars/org.slf4j_slf4j-api-1.7.32.jar,spark://41026342dcb2:36743/jars/org.apache.commons_commons-pool2-2.11.1.jar,spark://41026342dcb2:36743/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,spark://41026342dcb2:36743/jars/org.apache.kafka_kafka-clients-2.8.1.jar,spark://41026342dcb2:36743/jars/org.spark-project.spark_unused-1.0.0.jar,spark://41026342dcb2:36743/jars/com.google.code.findbugs_jsr305-3.0.0.jar,spark://41026342dcb2:36743/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,spark://41026342dcb2:36743/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,spark://41026342dcb2:36743/jars/org.lz4_lz4-java-1.8.0.jar,spark://41026342dcb2:36743/jars/commons-logging_commons-logging-1.1.3.jar, spark.executor.id=driver, spark.files=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.driver.host=41026342dcb2, spark.jars=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.submit.deployMode=client, spark.app.submitTime=1748881767372, spark.sql.execution.id=0, spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0, spark.master=local[*], spark.serializer.objectStreamReset=100, spark.repl.local.jars=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.submit.pyFiles=/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,/opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,/opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,/opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,/opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,/opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,/opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.app.startTime=1748881768847, spark.rdd.compress=True, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.app.id=local-1748881773047, spark.app.name=KafkaLogConsumerBatch, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.driver.port=36743}) by listener AppStatusListener took 1.181632998s.
[2025-06-02T16:31:14.317+0000] {subprocess.py:93} INFO - 25/06/02 16:31:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 262.5 KiB, free 366.0 MiB)
[2025-06-02T16:31:19.327+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 92.5 KiB, free 366.0 MiB)
[2025-06-02T16:31:19.734+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 41026342dcb2:46639 (size: 92.5 KiB, free: 366.2 MiB)
[2025-06-02T16:31:19.735+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
[2025-06-02T16:31:19.735+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-02T16:31:19.736+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-06-02T16:31:19.768+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (41026342dcb2, executor driver, partition 0, PROCESS_LOCAL, 4631 bytes) taskResourceAssignments Map()
[2025-06-02T16:31:19.824+0000] {subprocess.py:93} INFO - 25/06/02 16:31:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-06-02T16:31:23.927+0000] {job.py:229} INFO - Heartbeat recovered after 13.41 seconds
[2025-06-02T16:31:34.624+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO ConsumerConfig: ConsumerConfig values:
[2025-06-02T16:31:34.625+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-06-02T16:31:34.625+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-06-02T16:31:34.626+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-06-02T16:31:34.626+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-06-02T16:31:34.627+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-06-02T16:31:34.627+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-06-02T16:31:34.628+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2
[2025-06-02T16:31:34.628+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-06-02T16:31:34.628+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-06-02T16:31:34.629+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-06-02T16:31:34.629+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-06-02T16:31:34.630+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-06-02T16:31:34.630+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-06-02T16:31:34.631+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-06-02T16:31:34.631+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-06-02T16:31:34.631+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor
[2025-06-02T16:31:34.632+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-06-02T16:31:34.632+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-06-02T16:31:34.632+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-06-02T16:31:34.633+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-06-02T16:31:34.633+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-06-02T16:31:34.634+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-06-02T16:31:34.634+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T16:31:34.634+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-06-02T16:31:34.635+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-06-02T16:31:34.635+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-06-02T16:31:34.636+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-06-02T16:31:34.636+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-06-02T16:31:34.636+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-06-02T16:31:34.637+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-06-02T16:31:34.637+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-06-02T16:31:34.637+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[2025-06-02T16:31:34.637+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-06-02T16:31:34.638+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-06-02T16:31:34.638+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-06-02T16:31:34.639+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-06-02T16:31:34.639+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-06-02T16:31:34.639+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-06-02T16:31:34.639+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-06-02T16:31:34.640+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-06-02T16:31:34.640+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-06-02T16:31:34.640+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-06-02T16:31:34.641+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-06-02T16:31:34.641+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-06-02T16:31:34.641+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-06-02T16:31:34.642+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-06-02T16:31:34.642+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-06-02T16:31:34.642+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-06-02T16:31:34.642+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-06-02T16:31:34.643+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-06-02T16:31:34.643+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-06-02T16:31:34.643+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-06-02T16:31:34.643+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-06-02T16:31:34.644+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-06-02T16:31:34.644+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 10000
[2025-06-02T16:31:34.644+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-06-02T16:31:34.644+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-06-02T16:31:34.645+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-06-02T16:31:34.645+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2]
[2025-06-02T16:31:34.645+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-06-02T16:31:34.645+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-06-02T16:31:34.646+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-06-02T16:31:34.646+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-06-02T16:31:34.646+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-06-02T16:31:34.646+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-06-02T16:31:34.647+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-06-02T16:31:34.647+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-06-02T16:31:34.647+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-06-02T16:31:34.648+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.2
[2025-06-02T16:31:34.648+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-06-02T16:31:34.648+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-06-02T16:31:34.648+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-06-02T16:31:34.649+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-06-02T16:31:34.649+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-06-02T16:31:34.649+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-06-02T16:31:34.649+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-06-02T16:31:34.650+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T16:31:34.650+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:31:34.650+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO AppInfoParser: Kafka version: 2.8.1
[2025-06-02T16:31:34.651+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2025-06-02T16:31:34.651+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO AppInfoParser: Kafka startTimeMs: 1748881894629
[2025-06-02T16:31:34.651+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Subscribed to partition(s): logs-0
[2025-06-02T16:31:34.651+0000] {subprocess.py:93} INFO - 25/06/02 16:31:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Seeking to EARLIEST offset of partition logs-0
[2025-06-02T16:32:14.718+0000] {job.py:239} ERROR - Job heartbeat failed with error
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py", line 233, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/local_task_job_runner.py", line 284, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2358, in refresh_from_db
    _refresh_from_db(task_instance=self, session=session, lock_for_update=lock_for_update)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 872, in _refresh_from_db
    ti = TaskInstance.get_task_instance(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2344, in get_task_instance
    return query.one_or_none()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-06-02T16:32:16.399+0000] {job.py:254} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-06-02T16:32:16.957+0000] {subprocess.py:93} INFO - 25/06/02 16:32:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:32:31.508+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:32:31.508+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka: Temporary failure in name resolution
[2025-06-02T16:32:31.509+0000] {subprocess.py:93} INFO - 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
[2025-06-02T16:32:31.509+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:866)
[2025-06-02T16:32:31.509+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1288)
[2025-06-02T16:32:31.510+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:815)
[2025-06-02T16:32:31.510+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:32:31.511+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:32:31.511+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:32:31.512+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:32:31.512+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:32:31.513+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:32:31.513+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:32:31.513+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:32:31.514+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:32:31.514+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:32:31.515+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:32:31.515+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:32:31.515+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:32:31.516+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:32:31.516+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:32:31.516+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:32:31.516+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:32:31.517+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:32:31.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:32:31.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:32:31.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:32:31.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:32:31.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:32:31.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:32:31.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:32:31.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:32:31.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.635+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.635+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.635+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:32:31.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:32:31.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:32:31.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:32:31.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:32:31.641+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:32:31.649+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:32:31.649+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:32:31.649+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:32:31.650+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:32:31.650+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka
[2025-06-02T16:32:31.651+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)
[2025-06-02T16:32:31.651+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:32:31.651+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:32:31.652+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:32:31.652+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:32:31.652+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:32:31.653+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:32:31.653+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:32:31.653+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:32:31.654+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:32:31.654+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:32:31.655+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:32:31.655+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:32:31.655+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:32:31.656+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:32:31.656+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:32:31.656+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:32:31.657+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:32:31.657+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:32:31.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:32:31.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:32:31.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:32:31.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:32:31.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:32:31.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:32:31.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:32:31.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:32:31.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:32:31.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:32:31.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:32:31.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:32:31.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:32:31.671+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:32:31.672+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:32:31.672+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:32:31.672+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:32:31.772+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:32:31.773+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka
[2025-06-02T16:32:31.773+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)
[2025-06-02T16:32:31.774+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:32:31.774+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:32:31.774+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:32:31.775+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:32:31.775+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:32:31.804+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:32:31.804+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:32:31.805+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:32:31.805+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:32:31.806+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:32:31.806+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:32:31.807+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:32:31.807+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:32:31.808+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:32:31.808+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:32:31.809+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:32:31.809+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:32:31.809+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:32:31.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:32:31.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:32:31.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:32:31.811+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:32:31.811+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:32:31.811+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:32:31.811+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:32:31.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:32:31.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.816+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.816+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.816+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.816+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:31.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:31.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:31.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:32:32.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:32:32.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:32:32.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:32:32.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:32:32.093+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:32:32.093+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:32:32.093+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:32:32.094+0000] {subprocess.py:93} INFO - 25/06/02 16:32:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:32:32.175+0000] {subprocess.py:93} INFO - 25/06/02 16:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:32:32.175+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka
[2025-06-02T16:32:32.176+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)
[2025-06-02T16:32:32.176+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:32:32.177+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:32:32.177+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:32:32.177+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:32:32.178+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:32:32.178+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:32:32.178+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:32:32.179+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:32:32.179+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:32:32.180+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:32:32.180+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:32:32.180+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:32:32.181+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:32:32.181+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:32:32.181+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:32:32.182+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:32:32.182+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:32:32.182+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:32:32.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:32:32.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:32:32.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:32:32.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:32:32.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:32:32.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:32:32.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:32:32.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:32:32.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:32.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:32.193+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:32.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:32:32.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:32:32.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:32:32.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:32:32.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:32:32.195+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:32:32.196+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:32:32.196+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:32:32.196+0000] {subprocess.py:93} INFO - 25/06/02 16:32:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:32:33.080+0000] {subprocess.py:93} INFO - 25/06/02 16:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:32:33.081+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka
[2025-06-02T16:32:33.082+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)
[2025-06-02T16:32:33.082+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:32:33.083+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:32:33.083+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:32:33.083+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:32:33.084+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:32:33.084+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:32:33.084+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:32:33.085+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:32:33.085+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:32:33.085+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:32:33.085+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:32:33.086+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:32:33.086+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:32:33.086+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:32:33.086+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:32:33.087+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:32:33.087+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:32:33.087+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:32:33.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:32:33.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:32:33.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:32:33.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:32:33.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:32:33.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:32:33.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:32:33.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:32:33.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.094+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.094+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.094+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:32:33.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:32:33.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:32:33.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:32:33.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:32:33.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:32:33.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:32:33.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:32:33.099+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:32:33.099+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:32:33.100+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:32:33.100+0000] {subprocess.py:93} INFO - 25/06/02 16:32:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:33:19.558+0000] {job.py:229} INFO - Heartbeat recovered after 79.95 seconds
[2025-06-02T16:33:38.941+0000] {subprocess.py:93} INFO - 25/06/02 16:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Error connecting to node kafka:9092 (id: -1 rack: null)
[2025-06-02T16:33:38.942+0000] {subprocess.py:93} INFO - java.net.UnknownHostException: kafka
[2025-06-02T16:33:38.942+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)
[2025-06-02T16:33:38.942+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
[2025-06-02T16:33:38.943+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1136)
[2025-06-02T16:33:38.943+0000] {subprocess.py:93} INFO - 	at java.net.InetAddress.getAllByName(InetAddress.java:1064)
[2025-06-02T16:33:38.944+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)
[2025-06-02T16:33:38.944+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)
[2025-06-02T16:33:38.944+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:513)
[2025-06-02T16:33:38.945+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:467)
[2025-06-02T16:33:38.945+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)
[2025-06-02T16:33:38.945+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)
[2025-06-02T16:33:38.946+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)
[2025-06-02T16:33:38.946+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)
[2025-06-02T16:33:38.946+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)
[2025-06-02T16:33:38.946+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)
[2025-06-02T16:33:38.947+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)
[2025-06-02T16:33:38.947+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
[2025-06-02T16:33:38.947+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)
[2025-06-02T16:33:38.948+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1746)
[2025-06-02T16:33:38.948+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1704)
[2025-06-02T16:33:38.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:109)
[2025-06-02T16:33:38.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$getAvailableOffsetRange$1(KafkaDataConsumer.scala:360)
[2025-06-02T16:33:38.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
[2025-06-02T16:33:38.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:618)
[2025-06-02T16:33:38.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:358)
[2025-06-02T16:33:38.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.resolveRange(KafkaSourceRDD.scala:123)
[2025-06-02T16:33:38.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaSourceRDD.compute(KafkaSourceRDD.scala:75)
[2025-06-02T16:33:38.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)
[2025-06-02T16:33:38.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-06-02T16:33:38.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
[2025-06-02T16:33:38.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
[2025-06-02T16:33:38.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2025-06-02T16:33:38.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2025-06-02T16:33:38.961+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2025-06-02T16:33:38.961+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2025-06-02T16:33:38.961+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2025-06-02T16:33:38.962+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T16:33:38.962+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T16:33:38.962+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:33:38.962+0000] {subprocess.py:93} INFO - 25/06/02 16:32:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor-2, groupId=spark-kafka-relation-f8d96f49-ec44-49c6-8da4-f88d089351ea-executor] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected
[2025-06-02T16:33:38.963+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2025-06-02T16:33:38.963+0000] {subprocess.py:93} INFO - org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:38.963+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (41026342dcb2 executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:38.964+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:33:38.964+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2025-06-02T16:33:38.964+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-06-02T16:33:38.964+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-06-02T16:33:38.965+0000] {subprocess.py:93} INFO - 25/06/02 16:32:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
[2025-06-02T16:33:38.965+0000] {subprocess.py:93} INFO - 25/06/02 16:32:46 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) failed in 95.446 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (41026342dcb2 executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:38.965+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:33:38.966+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-06-02T16:33:38.966+0000] {subprocess.py:93} INFO - 25/06/02 16:32:46 INFO DAGScheduler: Job 0 failed: json at NativeMethodAccessorImpl.java:0, took 95.901133 s
[2025-06-02T16:33:39.410+0000] {subprocess.py:93} INFO - 25/06/02 16:32:46 ERROR FileFormatWriter: Aborting job 2f812d27-47ed-46ef-9c1d-0ac470bb59dc.
[2025-06-02T16:33:39.411+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (41026342dcb2 executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:39.411+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:33:39.411+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-06-02T16:33:39.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2025-06-02T16:33:39.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2025-06-02T16:33:39.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2025-06-02T16:33:39.412+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-06-02T16:33:39.413+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-06-02T16:33:39.413+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-06-02T16:33:39.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2025-06-02T16:33:39.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.414+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-06-02T16:33:39.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2025-06-02T16:33:39.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2025-06-02T16:33:39.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2025-06-02T16:33:39.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-06-02T16:33:39.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2025-06-02T16:33:39.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2025-06-02T16:33:39.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)
[2025-06-02T16:33:39.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2025-06-02T16:33:39.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-06-02T16:33:39.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-06-02T16:33:39.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-06-02T16:33:39.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-06-02T16:33:39.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2025-06-02T16:33:39.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2025-06-02T16:33:39.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2025-06-02T16:33:39.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2025-06-02T16:33:39.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2025-06-02T16:33:39.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-06-02T16:33:39.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-06-02T16:33:39.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2025-06-02T16:33:39.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2025-06-02T16:33:39.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2025-06-02T16:33:39.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-06-02T16:33:39.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-06-02T16:33:39.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2025-06-02T16:33:39.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-06-02T16:33:39.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-06-02T16:33:39.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-06-02T16:33:39.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2025-06-02T16:33:39.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2025-06-02T16:33:39.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2025-06-02T16:33:39.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2025-06-02T16:33:39.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2025-06-02T16:33:39.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:775)
[2025-06-02T16:33:39.477+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-06-02T16:33:39.477+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-06-02T16:33:39.477+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-06-02T16:33:39.477+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2025-06-02T16:33:39.478+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-06-02T16:33:39.478+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2025-06-02T16:33:39.478+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-06-02T16:33:39.479+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-06-02T16:33:39.479+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-06-02T16:33:39.479+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-06-02T16:33:39.480+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-06-02T16:33:39.480+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:33:39.480+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:39.481+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-06-02T16:33:39.481+0000] {subprocess.py:93} INFO -   File "/app/log_consumer.py", line 34, in <module>
[2025-06-02T16:33:39.481+0000] {subprocess.py:93} INFO -     error_logs.write.mode("append").json("/app/output/error_logs")
[2025-06-02T16:33:39.482+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1095, in json
[2025-06-02T16:33:39.482+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2025-06-02T16:33:39.482+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2025-06-02T16:33:39.483+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-06-02T16:33:39.483+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o53.json.
[2025-06-02T16:33:39.483+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted.
[2025-06-02T16:33:39.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)
[2025-06-02T16:33:39.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)
[2025-06-02T16:33:39.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2025-06-02T16:33:39.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-06-02T16:33:39.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-06-02T16:33:39.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-06-02T16:33:39.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2025-06-02T16:33:39.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2025-06-02T16:33:39.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2025-06-02T16:33:39.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2025-06-02T16:33:39.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2025-06-02T16:33:39.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2025-06-02T16:33:39.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-06-02T16:33:39.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2025-06-02T16:33:39.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2025-06-02T16:33:39.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2025-06-02T16:33:39.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2025-06-02T16:33:39.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-06-02T16:33:39.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-06-02T16:33:39.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2025-06-02T16:33:39.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2025-06-02T16:33:39.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2025-06-02T16:33:39.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2025-06-02T16:33:39.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2025-06-02T16:33:39.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2025-06-02T16:33:39.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2025-06-02T16:33:39.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2025-06-02T16:33:39.493+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2025-06-02T16:33:39.493+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2025-06-02T16:33:39.493+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:775)
[2025-06-02T16:33:39.494+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-06-02T16:33:39.494+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-06-02T16:33:39.494+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-06-02T16:33:39.495+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2025-06-02T16:33:39.495+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-06-02T16:33:39.495+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2025-06-02T16:33:39.496+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-06-02T16:33:39.496+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-06-02T16:33:39.496+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-06-02T16:33:39.497+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-06-02T16:33:39.497+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-06-02T16:33:39.497+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T16:33:39.498+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (41026342dcb2 executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:39.498+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:33:39.498+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-06-02T16:33:39.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2025-06-02T16:33:39.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2025-06-02T16:33:39.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2025-06-02T16:33:39.500+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-06-02T16:33:39.500+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-06-02T16:33:39.500+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-06-02T16:33:39.500+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2025-06-02T16:33:39.501+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.501+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.501+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-06-02T16:33:39.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2025-06-02T16:33:39.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2025-06-02T16:33:39.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2025-06-02T16:33:39.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2025-06-02T16:33:39.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-06-02T16:33:39.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2025-06-02T16:33:39.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2025-06-02T16:33:39.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)
[2025-06-02T16:33:39.505+0000] {subprocess.py:93} INFO - 	... 42 more
[2025-06-02T16:33:39.505+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition logs-0 could be determined
[2025-06-02T16:33:39.505+0000] {subprocess.py:93} INFO - 
[2025-06-02T16:33:39.506+0000] {subprocess.py:93} INFO - 25/06/02 16:32:46 INFO SparkContext: Invoking stop() from shutdown hook
[2025-06-02T16:33:39.506+0000] {subprocess.py:93} INFO - 25/06/02 16:32:46 INFO SparkUI: Stopped Spark web UI at http://41026342dcb2:4040
[2025-06-02T16:33:39.506+0000] {subprocess.py:93} INFO - 25/06/02 16:32:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-02T16:33:39.507+0000] {subprocess.py:93} INFO - 25/06/02 16:32:47 INFO MemoryStore: MemoryStore cleared
[2025-06-02T16:33:39.507+0000] {subprocess.py:93} INFO - 25/06/02 16:32:47 INFO BlockManager: BlockManager stopped
[2025-06-02T16:33:39.508+0000] {subprocess.py:93} INFO - 25/06/02 16:32:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-02T16:33:39.508+0000] {subprocess.py:93} INFO - 25/06/02 16:32:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-02T16:33:39.508+0000] {subprocess.py:93} INFO - 25/06/02 16:32:48 INFO SparkContext: Successfully stopped SparkContext
[2025-06-02T16:33:39.509+0000] {subprocess.py:93} INFO - 25/06/02 16:32:48 INFO ShutdownHookManager: Shutdown hook called
[2025-06-02T16:33:39.552+0000] {subprocess.py:93} INFO - 25/06/02 16:32:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964/pyspark-8c81aee4-d509-493b-b7b0-fdca072c4b5c
[2025-06-02T16:33:39.552+0000] {subprocess.py:93} INFO - 25/06/02 16:32:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-829f5927-6081-4d95-b1f7-f6537b3b4076
[2025-06-02T16:33:39.553+0000] {subprocess.py:93} INFO - 25/06/02 16:32:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-86ee528b-5238-453d-bbe4-1042e3cfa964
[2025-06-02T16:33:38.996+0000] {job.py:229} INFO - Heartbeat recovered after 75.77 seconds
[2025-06-02T16:33:40.986+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-06-02T16:33:58.086+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-06-02T16:33:58.114+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=log_monitoring_dag, task_id=start_spark_consumer, run_id=manual__2025-06-02T15:37:47.618112+00:00, execution_date=20250602T153747, start_date=20250602T162919, end_date=20250602T163358
[2025-06-02T16:33:59.262+0000] {job.py:229} INFO - Heartbeat recovered after 34.60 seconds
[2025-06-02T16:33:59.268+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-06-02T16:33:59.321+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 126 for task start_spark_consumer (Bash command failed. The command returned a non-zero exit code 1.; 2012)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-06-02T16:33:59.795+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-06-02T16:34:00.220+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
