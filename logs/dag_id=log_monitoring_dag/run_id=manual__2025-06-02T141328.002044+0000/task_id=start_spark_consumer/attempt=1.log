[2025-06-02T14:18:31.264+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-06-02T14:18:31.281+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T14:13:28.002044+00:00 [queued]>
[2025-06-02T14:18:31.290+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T14:13:28.002044+00:00 [queued]>
[2025-06-02T14:18:31.292+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2025-06-02T14:18:31.309+0000] {taskinstance.py:2888} INFO - Executing <Task(BashOperator): start_spark_consumer> on 2025-06-02 14:13:28.002044+00:00
[2025-06-02T14:18:31.330+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=1146) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-06-02T14:18:31.332+0000] {standard_task_runner.py:72} INFO - Started process 1147 to run task
[2025-06-02T14:18:31.331+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'log_monitoring_dag', 'start_spark_consumer', 'manual__2025-06-02T14:13:28.002044+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/log_monitoring_dag.py', '--cfg-path', '/tmp/tmpgdhp6qtj']
[2025-06-02T14:18:31.334+0000] {standard_task_runner.py:105} INFO - Job 113: Subtask start_spark_consumer
[2025-06-02T14:18:31.354+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:209 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-06-02T14:18:31.387+0000] {task_command.py:467} INFO - Running <TaskInstance: log_monitoring_dag.start_spark_consumer manual__2025-06-02T14:13:28.002044+00:00 [running]> on host c47ce43ea693
[2025-06-02T14:18:31.487+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='log_monitoring_dag' AIRFLOW_CTX_TASK_ID='start_spark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-06-02T14:13:28.002044+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-02T14:13:28.002044+00:00'
[2025-06-02T14:18:31.487+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-06-02T14:18:31.508+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-06-02T14:18:31.517+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'bash -c "echo Starting Spark Consumer && docker exec realtime_log_monitoring-spark-1 spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /app/log_consumer.py && echo Finished Spark Consumer"']
[2025-06-02T14:18:31.532+0000] {subprocess.py:86} INFO - Output:
[2025-06-02T14:18:31.574+0000] {subprocess.py:93} INFO - Starting Spark Consumer
[2025-06-02T14:19:34.449+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-06-02T14:19:34.898+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-06-02T14:19:34.898+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-06-02T14:19:34.903+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-06-02T14:19:34.904+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-ed7b25d3-2c4b-4cbb-b1dc-3152b039565f;1.0
[2025-06-02T14:19:34.905+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-06-02T14:19:35.392+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2025-06-02T14:19:35.514+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2025-06-02T14:19:35.690+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;2.8.1 in central
[2025-06-02T14:19:35.715+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-06-02T14:19:35.848+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2025-06-02T14:19:35.921+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;1.7.32 in central
[2025-06-02T14:19:36.190+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2025-06-02T14:19:36.233+0000] {subprocess.py:93} INFO - 	found org.spark-project.spark#unused;1.0.0 in central
[2025-06-02T14:19:36.480+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2025-06-02T14:19:36.606+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-06-02T14:19:36.645+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-06-02T14:19:36.656+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-06-02T14:19:37.089+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1833ms :: artifacts dl 351ms
[2025-06-02T14:19:37.090+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-06-02T14:19:37.090+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-06-02T14:19:37.091+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-06-02T14:19:37.091+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-06-02T14:19:37.092+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
[2025-06-02T14:19:37.092+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
[2025-06-02T14:19:37.093+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
[2025-06-02T14:19:37.094+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
[2025-06-02T14:19:37.094+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
[2025-06-02T14:19:37.095+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-06-02T14:19:37.095+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;1.7.32 from central in [default]
[2025-06-02T14:19:37.096+0000] {subprocess.py:93} INFO - 	org.spark-project.spark#unused;1.0.0 from central in [default]
[2025-06-02T14:19:37.096+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
[2025-06-02T14:19:37.097+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T14:19:37.097+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-06-02T14:19:37.097+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-06-02T14:19:37.098+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T14:19:37.098+0000] {subprocess.py:93} INFO - 	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
[2025-06-02T14:19:37.098+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-06-02T14:19:37.195+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-ed7b25d3-2c4b-4cbb-b1dc-3152b039565f
[2025-06-02T14:19:37.196+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-06-02T14:19:37.298+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 12 already retrieved (0kB/103ms)
[2025-06-02T14:19:39.848+0000] {subprocess.py:93} INFO - 25/06/02 14:19:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-02T14:19:56.484+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO SparkContext: Running Spark version 3.3.0
[2025-06-02T14:19:56.660+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceUtils: ==============================================================
[2025-06-02T14:19:56.661+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-02T14:19:56.662+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceUtils: ==============================================================
[2025-06-02T14:19:56.662+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO SparkContext: Submitted application: KafkaLogConsumerBatch
[2025-06-02T14:19:57.070+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-02T14:19:57.070+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceProfile: Limiting resource is cpu
[2025-06-02T14:19:57.071+0000] {subprocess.py:93} INFO - 25/06/02 14:19:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-02T14:19:57.781+0000] {subprocess.py:93} INFO - 25/06/02 14:19:57 INFO SecurityManager: Changing view acls to: spark
[2025-06-02T14:19:57.782+0000] {subprocess.py:93} INFO - 25/06/02 14:19:57 INFO SecurityManager: Changing modify acls to: spark
[2025-06-02T14:19:57.782+0000] {subprocess.py:93} INFO - 25/06/02 14:19:57 INFO SecurityManager: Changing view acls groups to:
[2025-06-02T14:19:57.783+0000] {subprocess.py:93} INFO - 25/06/02 14:19:57 INFO SecurityManager: Changing modify acls groups to:
[2025-06-02T14:19:57.783+0000] {subprocess.py:93} INFO - 25/06/02 14:19:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
[2025-06-02T14:19:59.592+0000] {subprocess.py:93} INFO - 25/06/02 14:19:59 INFO Utils: Successfully started service 'sparkDriver' on port 36457.
[2025-06-02T14:20:00.124+0000] {subprocess.py:93} INFO - 25/06/02 14:20:00 INFO SparkEnv: Registering MapOutputTracker
[2025-06-02T14:20:00.517+0000] {subprocess.py:93} INFO - 25/06/02 14:20:00 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-02T14:20:00.764+0000] {subprocess.py:93} INFO - 25/06/02 14:20:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-02T14:20:00.765+0000] {subprocess.py:93} INFO - 25/06/02 14:20:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-02T14:20:00.792+0000] {subprocess.py:93} INFO - 25/06/02 14:20:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-02T14:20:01.279+0000] {subprocess.py:93} INFO - 25/06/02 14:20:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7f9c1a5d-83c7-4cb2-8e64-0ecf247a00a2
[2025-06-02T14:20:01.509+0000] {subprocess.py:93} INFO - 25/06/02 14:20:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2025-06-02T14:20:01.823+0000] {subprocess.py:93} INFO - 25/06/02 14:20:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-02T14:20:03.073+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-02T14:20:03.148+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://41026342dcb2:36457/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.150+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://41026342dcb2:36457/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.169+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://41026342dcb2:36457/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748873996315
[2025-06-02T14:20:03.183+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://41026342dcb2:36457/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.195+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://41026342dcb2:36457/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748873996315
[2025-06-02T14:20:03.196+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://41026342dcb2:36457/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.196+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:03.197+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://41026342dcb2:36457/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.197+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://41026342dcb2:36457/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748873996315
[2025-06-02T14:20:03.197+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://41026342dcb2:36457/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748873996315
[2025-06-02T14:20:03.198+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:03.198+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://41026342dcb2:36457/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748873996315
[2025-06-02T14:20:03.199+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:03.199+0000] {subprocess.py:93} INFO - 25/06/02 14:20:03 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:20:04.126+0000] {subprocess.py:93} INFO - 25/06/02 14:20:04 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:04.127+0000] {subprocess.py:93} INFO - 25/06/02 14:20:04 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:20:04.493+0000] {subprocess.py:93} INFO - 25/06/02 14:20:04 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748873996315
[2025-06-02T14:20:04.493+0000] {subprocess.py:93} INFO - 25/06/02 14:20:04 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T14:20:10.212+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:10.213+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T14:20:10.487+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748873996315
[2025-06-02T14:20:10.488+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T14:20:10.712+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:10.712+0000] {subprocess.py:93} INFO - 25/06/02 14:20:10 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T14:20:11.122+0000] {subprocess.py:93} INFO - 25/06/02 14:20:11 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:11.123+0000] {subprocess.py:93} INFO - 25/06/02 14:20:11 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T14:20:22.058+0000] {subprocess.py:93} INFO - 25/06/02 14:20:22 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748873996315
[2025-06-02T14:20:22.059+0000] {subprocess.py:93} INFO - 25/06/02 14:20:22 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T14:20:23.334+0000] {subprocess.py:93} INFO - 25/06/02 14:20:23 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748873996315
[2025-06-02T14:20:23.335+0000] {subprocess.py:93} INFO - 25/06/02 14:20:23 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T14:20:25.454+0000] {subprocess.py:93} INFO - 25/06/02 14:20:25 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748873996315
[2025-06-02T14:20:25.454+0000] {subprocess.py:93} INFO - 25/06/02 14:20:25 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T14:20:25.733+0000] {subprocess.py:93} INFO - 25/06/02 14:20:25 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:25.733+0000] {subprocess.py:93} INFO - 25/06/02 14:20:25 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T14:20:34.562+0000] {subprocess.py:93} INFO - 25/06/02 14:20:34 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748873996315
[2025-06-02T14:20:34.563+0000] {subprocess.py:93} INFO - 25/06/02 14:20:34 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T14:20:36.508+0000] {subprocess.py:93} INFO - 25/06/02 14:20:36 INFO Executor: Starting executor ID driver on host 41026342dcb2
[2025-06-02T14:20:36.597+0000] {subprocess.py:93} INFO - 25/06/02 14:20:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-06-02T14:20:36.840+0000] {subprocess.py:93} INFO - 25/06/02 14:20:36 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748873996315
[2025-06-02T14:20:36.900+0000] {subprocess.py:93} INFO - 25/06/02 14:20:36 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T14:20:37.130+0000] {subprocess.py:93} INFO - 25/06/02 14:20:37 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:37.156+0000] {subprocess.py:93} INFO - 25/06/02 14:20:37 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T14:20:38.138+0000] {subprocess.py:93} INFO - 25/06/02 14:20:38 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748873996315
[2025-06-02T14:20:38.139+0000] {subprocess.py:93} INFO - 25/06/02 14:20:38 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T14:20:38.725+0000] {subprocess.py:93} INFO - 25/06/02 14:20:38 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748873996315
[2025-06-02T14:20:38.726+0000] {subprocess.py:93} INFO - 25/06/02 14:20:38 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T14:20:39.164+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748873996315
[2025-06-02T14:20:39.184+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T14:20:39.413+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748873996315
[2025-06-02T14:20:39.420+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T14:20:39.816+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:39.817+0000] {subprocess.py:93} INFO - 25/06/02 14:20:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:20:40.269+0000] {subprocess.py:93} INFO - 25/06/02 14:20:40 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:40.269+0000] {subprocess.py:93} INFO - 25/06/02 14:20:40 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T14:20:40.687+0000] {subprocess.py:93} INFO - 25/06/02 14:20:40 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748873996315
[2025-06-02T14:20:40.689+0000] {subprocess.py:93} INFO - 25/06/02 14:20:40 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T14:20:41.014+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:41.032+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T14:20:41.308+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:20:41.308+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:20:41.675+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748873996315
[2025-06-02T14:20:41.676+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T14:20:41.748+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748873996315
[2025-06-02T14:20:41.916+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO TransportClientFactory: Successfully created connection to 41026342dcb2/172.18.0.7:36457 after 103 ms (0 ms spent in bootstraps)
[2025-06-02T14:20:41.959+0000] {subprocess.py:93} INFO - 25/06/02 14:20:41 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp538811773650065934.tmp
[2025-06-02T14:20:42.176+0000] {subprocess.py:93} INFO - 25/06/02 14:20:42 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp538811773650065934.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-02T14:20:42.396+0000] {subprocess.py:93} INFO - 25/06/02 14:20:42 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.slf4j_slf4j-api-1.7.32.jar to class loader
[2025-06-02T14:20:42.398+0000] {subprocess.py:93} INFO - 25/06/02 14:20:42 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:20:42.398+0000] {subprocess.py:93} INFO - 25/06/02 14:20:42 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3309282523599661524.tmp
[2025-06-02T14:21:23.769+0000] {job.py:229} INFO - Heartbeat recovered after 41.02 seconds
[2025-06-02T14:21:25.093+0000] {subprocess.py:93} INFO - 25/06/02 14:21:24 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:25.259+0000] {subprocess.py:93} INFO - 25/06/02 14:21:24 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:25.386+0000] {subprocess.py:93} INFO - 25/06/02 14:21:24 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:25.637+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 ERROR Inbox: Ignoring error
[2025-06-02T14:21:25.638+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:25.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.643+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.643+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.644+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.644+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.645+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.645+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.646+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:25.646+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:25.646+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:25.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:25.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:25.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:25.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:25.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:25.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:25.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:25.649+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:25.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:25.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:25.650+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.650+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:25.651+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:25.651+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:25.651+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.652+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.652+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.652+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:25.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.655+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.655+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.656+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:25.656+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:25.657+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:25.657+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:25.657+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 ERROR Inbox: Ignoring error
[2025-06-02T14:21:25.658+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:25.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.660+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.660+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.660+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.661+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.661+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.661+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:25.662+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:25.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:25.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:25.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:25.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:25.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:25.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:25.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:25.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:25.664+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:25.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:25.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:25.665+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.666+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:25.666+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:25.666+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:25.666+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.667+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.667+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.667+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:25.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.669+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.670+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.670+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:25.670+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:25.670+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:25.671+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:25.671+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 ERROR Inbox: Ignoring error
[2025-06-02T14:21:25.671+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:25.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.673+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.673+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.675+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.675+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.675+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.675+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.676+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.676+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:25.676+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:25.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:25.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:25.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:25.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:25.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:25.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:25.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:25.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:25.678+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:25.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:25.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:25.679+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.679+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:25.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:25.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:25.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.681+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.681+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:25.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.683+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.683+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.684+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:25.792+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:25.793+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:25.793+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:25.794+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 ERROR Inbox: Ignoring error
[2025-06-02T14:21:25.794+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:25.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.797+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.798+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.798+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.798+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.799+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.799+0000] {subprocess.py:93} INFO - 25/06/02 14:21:25 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:25.800+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:25.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:25.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:25.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:25.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:25.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:25.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:25.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:25.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:25.803+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:25.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:25.819+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:25.819+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.819+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:25.820+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:25.820+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:25.821+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:25.821+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:25.821+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:25.822+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:25.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:25.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:25.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:25.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:25.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:25.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:25.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:25.824+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:25.825+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:25.825+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:58.119+0000] {subprocess.py:93} INFO - 25/06/02 14:21:30 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:58.129+0000] {subprocess.py:93} INFO - 25/06/02 14:21:30 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:58.250+0000] {subprocess.py:93} INFO - 25/06/02 14:21:30 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:58.560+0000] {subprocess.py:93} INFO - 25/06/02 14:21:30 ERROR Inbox: Ignoring error
[2025-06-02T14:21:58.629+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:58.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:58.633+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.634+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:58.634+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.635+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.635+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.635+0000] {subprocess.py:93} INFO - 25/06/02 14:21:30 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:58.636+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:58.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:58.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:58.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:58.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:58.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:58.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:58.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:58.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:58.639+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:58.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:58.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:58.640+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.641+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:58.641+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:58.642+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:58.642+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.642+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.643+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.643+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:58.643+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.646+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:58.646+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.646+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:58.647+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:58.648+0000] {subprocess.py:93} INFO - 25/06/02 14:21:40 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:58.648+0000] {subprocess.py:93} INFO - 25/06/02 14:21:40 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:58.648+0000] {subprocess.py:93} INFO - 25/06/02 14:21:40 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:58.649+0000] {subprocess.py:93} INFO - 25/06/02 14:21:40 ERROR Inbox: Ignoring error
[2025-06-02T14:21:58.649+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:58.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:58.652+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.652+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:58.652+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.652+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.653+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.653+0000] {subprocess.py:93} INFO - 25/06/02 14:21:40 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:58.653+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:58.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:58.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:58.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:58.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:58.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:58.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:58.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:58.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:58.657+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:58.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:58.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:58.658+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.658+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:58.659+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:58.659+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:58.659+0000] {job.py:229} INFO - Heartbeat recovered after 66.47 seconds
[2025-06-02T14:21:58.660+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.660+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.661+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.662+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:58.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:58.666+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.667+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:58.667+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:58.667+0000] {subprocess.py:93} INFO - 25/06/02 14:21:42 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3309282523599661524.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-02T14:21:58.668+0000] {subprocess.py:93} INFO - 25/06/02 14:21:50 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:21:58.668+0000] {subprocess.py:93} INFO - 25/06/02 14:21:50 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:21:58.668+0000] {subprocess.py:93} INFO - 25/06/02 14:21:50 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:21:58.669+0000] {subprocess.py:93} INFO - 25/06/02 14:21:50 ERROR Inbox: Ignoring error
[2025-06-02T14:21:58.669+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:21:58.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:58.671+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.672+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:58.672+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.672+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.673+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.673+0000] {subprocess.py:93} INFO - 25/06/02 14:21:50 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:21:58.673+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:21:58.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:21:58.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:21:58.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:21:58.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:21:58.675+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:21:58.675+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:21:58.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:21:58.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:21:58.676+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:21:58.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:21:58.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:21:58.678+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:58.678+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:21:58.679+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:21:58.679+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:21:58.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:21:58.680+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:21:58.680+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:21:58.681+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:21:58.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:21:58.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:21:58.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:21:58.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:21:58.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:21:58.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:21:58.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:21:59.010+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:21:59.011+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:21:59.011+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:21:59.354+0000] {subprocess.py:93} INFO - 25/06/02 14:21:59 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader
[2025-06-02T14:21:59.355+0000] {subprocess.py:93} INFO - 25/06/02 14:21:59 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748873996315
[2025-06-02T14:22:00.343+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3180917599579285732.tmp
[2025-06-02T14:22:00.864+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:00.897+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:00.898+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:00.898+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 ERROR Inbox: Ignoring error
[2025-06-02T14:22:00.900+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:00.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:00.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:00.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:00.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:00.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:00.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:00.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:00.905+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:00.905+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:00.906+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:00.907+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:00.910+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:00.967+0000] {subprocess.py:93} INFO - 25/06/02 14:22:00 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:00.968+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:00.968+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:00.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:00.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:00.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:00.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:00.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:00.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:00.971+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:00.971+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:00.971+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:00.972+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:00.972+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:00.972+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:00.972+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:00.973+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:00.973+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:00.973+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:00.974+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:00.974+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:00.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:00.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:00.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:00.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:00.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:00.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:00.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:00.977+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:00.978+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:00.978+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:10.764+0000] {subprocess.py:93} INFO - 25/06/02 14:22:10 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:10.765+0000] {subprocess.py:93} INFO - 25/06/02 14:22:10 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:10.765+0000] {subprocess.py:93} INFO - 25/06/02 14:22:10 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:10.781+0000] {subprocess.py:93} INFO - 25/06/02 14:22:10 ERROR Inbox: Ignoring error
[2025-06-02T14:22:10.784+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:10.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:10.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:10.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:10.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:10.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:10.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:10.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:10.787+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:10.788+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:10.788+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:10.789+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:10.789+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:10.790+0000] {subprocess.py:93} INFO - 25/06/02 14:22:10 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:10.790+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:10.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:10.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:10.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:10.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:10.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:10.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:10.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:10.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:10.794+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:10.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:10.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:10.795+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:10.795+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:10.795+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:10.795+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:10.796+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:10.796+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:10.796+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:10.797+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:10.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:10.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:10.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:10.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:10.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:10.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:10.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:10.799+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:10.799+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:10.800+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:20.422+0000] {subprocess.py:93} INFO - 25/06/02 14:22:20 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:20.423+0000] {subprocess.py:93} INFO - 25/06/02 14:22:20 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:20.424+0000] {subprocess.py:93} INFO - 25/06/02 14:22:20 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:20.424+0000] {subprocess.py:93} INFO - 25/06/02 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:20.425+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:20.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:20.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:20.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:20.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:20.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:20.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:20.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:20.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:20.429+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:20.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:20.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:20.430+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:20.430+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:20.430+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:20.431+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:20.431+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:20.431+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:20.432+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:20.432+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:20.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:20.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:20.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:20.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:20.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:20.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:20.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:20.435+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:20.435+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:20.435+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:20.435+0000] {subprocess.py:93} INFO - 25/06/02 14:22:20 ERROR Inbox: Ignoring error
[2025-06-02T14:22:20.436+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:20.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:20.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:20.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:20.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:20.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:20.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:20.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:20.438+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:20.438+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:20.438+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:20.439+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:20.439+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:30.420+0000] {subprocess.py:93} INFO - 25/06/02 14:22:30 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:30.422+0000] {subprocess.py:93} INFO - 25/06/02 14:22:30 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:30.422+0000] {subprocess.py:93} INFO - 25/06/02 14:22:30 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:30.423+0000] {subprocess.py:93} INFO - 25/06/02 14:22:30 ERROR Inbox: Ignoring error
[2025-06-02T14:22:30.423+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:30.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:30.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:30.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:30.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:30.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:30.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:30.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:30.426+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:30.426+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:30.427+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:30.427+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:30.427+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:30.428+0000] {subprocess.py:93} INFO - 25/06/02 14:22:30 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:30.428+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:30.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:30.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:30.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:30.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:30.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:30.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:30.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:30.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:30.431+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:30.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:30.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:30.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:30.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:30.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:30.433+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:30.433+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:30.433+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:30.434+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:30.434+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:30.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:30.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:30.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:30.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:30.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:30.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:30.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:30.436+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:30.436+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:30.437+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:35.889+0000] {job.py:229} INFO - Heartbeat recovered after 29.55 seconds
[2025-06-02T14:22:40.420+0000] {subprocess.py:93} INFO - 25/06/02 14:22:40 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:40.426+0000] {subprocess.py:93} INFO - 25/06/02 14:22:40 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:40.445+0000] {subprocess.py:93} INFO - 25/06/02 14:22:40 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:40.446+0000] {subprocess.py:93} INFO - 25/06/02 14:22:40 ERROR Inbox: Ignoring error
[2025-06-02T14:22:40.446+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:40.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:40.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:40.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:40.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:40.449+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:40.449+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:40.450+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:40.450+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:40.451+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:40.451+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:40.452+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:40.452+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:40.453+0000] {subprocess.py:93} INFO - 25/06/02 14:22:40 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:40.453+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:40.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:40.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:40.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:40.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:40.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:40.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:40.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:40.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:40.456+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:40.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:40.457+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:40.457+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:40.457+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:40.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:40.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:40.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:40.459+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:40.459+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:40.459+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:40.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:40.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:40.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:40.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:40.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:40.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:40.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:40.462+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:40.463+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:40.463+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:42.104+0000] {job.py:229} INFO - Heartbeat recovered after 23.90 seconds
[2025-06-02T14:22:42.938+0000] {subprocess.py:93} INFO - 25/06/02 14:22:42 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3180917599579285732.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-02T14:22:43.847+0000] {subprocess.py:93} INFO - 25/06/02 14:22:43 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader
[2025-06-02T14:22:43.848+0000] {subprocess.py:93} INFO - 25/06/02 14:22:43 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1748873996315
[2025-06-02T14:22:43.884+0000] {subprocess.py:93} INFO - 25/06/02 14:22:43 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3521633546024086580.tmp
[2025-06-02T14:22:43.886+0000] {subprocess.py:93} INFO - 25/06/02 14:22:43 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3521633546024086580.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.lz4_lz4-java-1.8.0.jar
[2025-06-02T14:22:44.882+0000] {subprocess.py:93} INFO - 25/06/02 14:22:44 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.lz4_lz4-java-1.8.0.jar to class loader
[2025-06-02T14:22:44.883+0000] {subprocess.py:93} INFO - 25/06/02 14:22:44 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1748873996315
[2025-06-02T14:22:44.884+0000] {subprocess.py:93} INFO - 25/06/02 14:22:44 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp8485908944867529573.tmp
[2025-06-02T14:22:44.885+0000] {subprocess.py:93} INFO - 25/06/02 14:22:44 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp8485908944867529573.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.commons_commons-pool2-2.11.1.jar
[2025-06-02T14:22:45.479+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.commons_commons-pool2-2.11.1.jar to class loader
[2025-06-02T14:22:45.480+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748873996315
[2025-06-02T14:22:45.584+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp5770364123082492836.tmp
[2025-06-02T14:22:45.586+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp5770364123082492836.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.spark-project.spark_unused-1.0.0.jar
[2025-06-02T14:22:45.928+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.spark-project.spark_unused-1.0.0.jar to class loader
[2025-06-02T14:22:45.931+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Executor: Fetching spark://41026342dcb2:36457/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748873996315
[2025-06-02T14:22:45.932+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Utils: Fetching spark://41026342dcb2:36457/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp8664849518633498241.tmp
[2025-06-02T14:22:45.932+0000] {subprocess.py:93} INFO - 25/06/02 14:22:45 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp8664849518633498241.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-02T14:22:47.041+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/com.google.code.findbugs_jsr305-3.0.0.jar to class loader
[2025-06-02T14:22:47.042+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748873996315
[2025-06-02T14:22:47.042+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2615754885709080.tmp
[2025-06-02T14:22:47.220+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2615754885709080.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-02T14:22:47.706+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader
[2025-06-02T14:22:47.706+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:22:47.751+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp5273051633043264843.tmp
[2025-06-02T14:22:47.754+0000] {subprocess.py:93} INFO - 25/06/02 14:22:47 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp5273051633043264843.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:22:48.453+0000] {subprocess.py:93} INFO - 25/06/02 14:22:48 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader
[2025-06-02T14:22:48.454+0000] {subprocess.py:93} INFO - 25/06/02 14:22:48 INFO Executor: Fetching spark://41026342dcb2:36457/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748873996315
[2025-06-02T14:22:48.580+0000] {subprocess.py:93} INFO - 25/06/02 14:22:48 INFO Utils: Fetching spark://41026342dcb2:36457/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3363402382469857214.tmp
[2025-06-02T14:22:48.582+0000] {subprocess.py:93} INFO - 25/06/02 14:22:48 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp3363402382469857214.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/commons-logging_commons-logging-1.1.3.jar
[2025-06-02T14:22:49.209+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/commons-logging_commons-logging-1.1.3.jar to class loader
[2025-06-02T14:22:49.210+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1748873996315
[2025-06-02T14:22:49.210+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2787973908224487226.tmp
[2025-06-02T14:22:49.286+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2787973908224487226.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.kafka_kafka-clients-2.8.1.jar
[2025-06-02T14:22:49.912+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.kafka_kafka-clients-2.8.1.jar to class loader
[2025-06-02T14:22:49.913+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Executor: Fetching spark://41026342dcb2:36457/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1748873996315
[2025-06-02T14:22:49.914+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Utils: Fetching spark://41026342dcb2:36457/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2873622480610953561.tmp
[2025-06-02T14:22:49.915+0000] {subprocess.py:93} INFO - 25/06/02 14:22:49 INFO Utils: /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/fetchFileTemp2873622480610953561.tmp has been previously copied to /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2025-06-02T14:22:50.419+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO Executor: Told to re-register on heartbeat
[2025-06-02T14:22:50.421+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManager: BlockManager null re-registering with master
[2025-06-02T14:22:50.422+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManagerMaster: Registering BlockManager null
[2025-06-02T14:22:50.422+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 ERROR Inbox: Ignoring error
[2025-06-02T14:22:50.423+0000] {subprocess.py:93} INFO - java.lang.NullPointerException
[2025-06-02T14:22:50.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:50.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:50.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:50.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:50.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:50.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:50.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:50.426+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:50.426+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:50.426+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:50.427+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:50.427+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:50.427+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 WARN Executor: Issue communicating with driver in heartbeater
[2025-06-02T14:22:50.428+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-06-02T14:22:50.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2025-06-02T14:22:50.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-06-02T14:22:50.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2025-06-02T14:22:50.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2025-06-02T14:22:50.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2025-06-02T14:22:50.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:633)
[2025-06-02T14:22:50.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2025-06-02T14:22:50.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2025-06-02T14:22:50.430+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-06-02T14:22:50.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2025-06-02T14:22:50.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2025-06-02T14:22:50.431+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:50.431+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
[2025-06-02T14:22:50.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
[2025-06-02T14:22:50.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
[2025-06-02T14:22:50.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2025-06-02T14:22:50.432+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2025-06-02T14:22:50.433+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2025-06-02T14:22:50.433+0000] {subprocess.py:93} INFO - Caused by: java.lang.NullPointerException
[2025-06-02T14:22:50.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2025-06-02T14:22:50.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2025-06-02T14:22:50.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2025-06-02T14:22:50.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-06-02T14:22:50.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-06-02T14:22:50.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-06-02T14:22:50.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-06-02T14:22:50.435+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2025-06-02T14:22:50.435+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2025-06-02T14:22:50.436+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-06-02T14:22:50.618+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO Executor: Adding file:/tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/userFiles-d4ea44c3-71ec-437f-861e-eb258d7ec4d7/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader
[2025-06-02T14:22:50.706+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46171.
[2025-06-02T14:22:50.707+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO NettyBlockTransferService: Server created on 41026342dcb2:46171
[2025-06-02T14:22:50.708+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-02T14:22:50.737+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 41026342dcb2, 46171, None)
[2025-06-02T14:22:50.739+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManagerMasterEndpoint: Registering block manager 41026342dcb2:46171 with 366.3 MiB RAM, BlockManagerId(driver, 41026342dcb2, 46171, None)
[2025-06-02T14:22:50.742+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 41026342dcb2, 46171, None)
[2025-06-02T14:22:50.744+0000] {subprocess.py:93} INFO - 25/06/02 14:22:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 41026342dcb2, 46171, None)
[2025-06-02T14:23:04.424+0000] {subprocess.py:93} INFO - 25/06/02 14:23:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-02T14:23:04.427+0000] {subprocess.py:93} INFO - 25/06/02 14:23:04 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-06-02T14:23:36.547+0000] {subprocess.py:93} INFO - 25/06/02 14:23:36 INFO CodeGenerator: Code generated in 3842.582683 ms
[2025-06-02T14:23:36.579+0000] {subprocess.py:93} INFO - 25/06/02 14:23:36 INFO CodeGenerator: Code generated in 22.409313 ms
[2025-06-02T14:24:21.438+0000] {job.py:229} INFO - Heartbeat recovered after 11.59 seconds
[2025-06-02T14:25:14.739+0000] {job.py:229} INFO - Heartbeat recovered after 59.86 seconds
[2025-06-02T14:25:20.683+0000] {job.py:229} INFO - Heartbeat recovered after 42.14 seconds
[2025-06-02T14:25:35.129+0000] {subprocess.py:93} INFO - 25/06/02 14:25:35 INFO ConsumerConfig: ConsumerConfig values:
[2025-06-02T14:25:35.130+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-06-02T14:25:35.131+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-06-02T14:25:35.131+0000] {subprocess.py:93} INFO - 	auto.offset.reset = earliest
[2025-06-02T14:25:35.131+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-06-02T14:25:35.132+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-06-02T14:25:35.132+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-06-02T14:25:35.132+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1
[2025-06-02T14:25:35.133+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-06-02T14:25:35.133+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-06-02T14:25:35.134+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-06-02T14:25:35.134+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-06-02T14:25:35.134+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-06-02T14:25:35.135+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-06-02T14:25:35.135+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-06-02T14:25:35.135+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-06-02T14:25:35.136+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0
[2025-06-02T14:25:35.136+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-06-02T14:25:35.137+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-06-02T14:25:35.137+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-06-02T14:25:35.137+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-06-02T14:25:35.137+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-06-02T14:25:35.138+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-06-02T14:25:35.138+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T14:25:35.138+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-06-02T14:25:35.139+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-06-02T14:25:35.139+0000] {subprocess.py:93} INFO - 	max.poll.records = 1
[2025-06-02T14:25:35.140+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-06-02T14:25:35.140+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-06-02T14:25:35.140+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-06-02T14:25:35.140+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-06-02T14:25:35.141+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-06-02T14:25:35.141+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[2025-06-02T14:25:35.141+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-06-02T14:25:35.142+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-06-02T14:25:35.142+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-06-02T14:25:35.142+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-06-02T14:25:35.143+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-06-02T14:25:35.143+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-06-02T14:25:35.143+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-06-02T14:25:35.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-06-02T14:25:35.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-06-02T14:25:35.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-06-02T14:25:35.145+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-06-02T14:25:35.145+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-06-02T14:25:35.145+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-06-02T14:25:35.146+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-06-02T14:25:35.146+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-06-02T14:25:35.146+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-06-02T14:25:35.147+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-06-02T14:25:35.147+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-06-02T14:25:35.147+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-06-02T14:25:35.148+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-06-02T14:25:35.148+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-06-02T14:25:35.148+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-06-02T14:25:35.149+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 10000
[2025-06-02T14:25:35.149+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-06-02T14:25:35.149+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-06-02T14:25:35.149+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-06-02T14:25:35.150+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2]
[2025-06-02T14:25:35.150+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-06-02T14:25:35.150+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-06-02T14:25:35.151+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-06-02T14:25:35.151+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-06-02T14:25:35.151+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-06-02T14:25:35.151+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-06-02T14:25:35.152+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-06-02T14:25:35.152+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-06-02T14:25:35.152+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-06-02T14:25:35.153+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.2
[2025-06-02T14:25:35.153+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-06-02T14:25:35.153+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-06-02T14:25:35.153+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-06-02T14:25:35.154+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-06-02T14:25:35.154+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-06-02T14:25:35.154+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-06-02T14:25:35.155+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-06-02T14:25:35.155+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T14:25:35.155+0000] {subprocess.py:93} INFO - 
[2025-06-02T14:25:36.492+0000] {subprocess.py:93} INFO - 25/06/02 14:25:36 INFO AppInfoParser: Kafka version: 2.8.1
[2025-06-02T14:25:36.493+0000] {subprocess.py:93} INFO - 25/06/02 14:25:36 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2025-06-02T14:25:36.494+0000] {subprocess.py:93} INFO - 25/06/02 14:25:36 INFO AppInfoParser: Kafka startTimeMs: 1748874336416
[2025-06-02T14:25:36.820+0000] {subprocess.py:93} INFO - 25/06/02 14:25:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Subscribed to topic(s): logs
[2025-06-02T14:25:37.576+0000] {subprocess.py:93} INFO - 25/06/02 14:25:37 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Cluster ID: KseVdgT7Th2WPRTGnxOBbw
[2025-06-02T14:25:37.694+0000] {subprocess.py:93} INFO - 25/06/02 14:25:37 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Discovered group coordinator kafka:9092 (id: 2147483646 rack: null)
[2025-06-02T14:25:37.698+0000] {subprocess.py:93} INFO - 25/06/02 14:25:37 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] (Re-)joining group
[2025-06-02T14:25:38.629+0000] {subprocess.py:93} INFO - 25/06/02 14:25:38 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] (Re-)joining group
[2025-06-02T14:25:43.043+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Successfully joined group with generation Generation{generationId=1, memberId='consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1-064263cc-d111-4dc5-a310-87447426cfc9', protocol='range'}
[2025-06-02T14:25:43.046+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1-064263cc-d111-4dc5-a310-87447426cfc9=Assignment(partitions=[logs-0])}
[2025-06-02T14:25:43.654+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Successfully synced group in generation Generation{generationId=1, memberId='consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1-064263cc-d111-4dc5-a310-87447426cfc9', protocol='range'}
[2025-06-02T14:25:43.655+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Notifying assignor about the new Assignment(partitions=[logs-0])
[2025-06-02T14:25:43.659+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Adding newly assigned partitions: logs-0
[2025-06-02T14:25:43.965+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Found no committed offset for partition logs-0
[2025-06-02T14:25:44.056+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Revoke previously assigned partitions logs-0
[2025-06-02T14:25:44.086+0000] {subprocess.py:93} INFO - 25/06/02 14:25:43 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0] Member consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1-064263cc-d111-4dc5-a310-87447426cfc9 sending LeaveGroup request to coordinator kafka:9092 (id: 2147483646 rack: null) due to the consumer is being closed
[2025-06-02T14:25:49.281+0000] {subprocess.py:93} INFO - 25/06/02 14:25:49 INFO Metrics: Metrics scheduler closed
[2025-06-02T14:25:49.281+0000] {subprocess.py:93} INFO - 25/06/02 14:25:49 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-06-02T14:25:49.282+0000] {subprocess.py:93} INFO - 25/06/02 14:25:49 INFO Metrics: Metrics reporters closed
[2025-06-02T14:25:49.285+0000] {subprocess.py:93} INFO - 25/06/02 14:25:49 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-driver-0-1 unregistered
[2025-06-02T14:25:49.287+0000] {subprocess.py:93} INFO - 25/06/02 14:25:49 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(logs-0,-2,-1,None)
[2025-06-02T14:25:53.367+0000] {subprocess.py:93} INFO - 25/06/02 14:25:53 INFO CodeGenerator: Code generated in 15.713745 ms
[2025-06-02T14:25:56.116+0000] {subprocess.py:93} INFO - 25/06/02 14:25:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-02T14:25:56.117+0000] {subprocess.py:93} INFO - 25/06/02 14:25:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-02T14:25:56.118+0000] {subprocess.py:93} INFO - 25/06/02 14:25:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2025-06-02T14:26:00.034+0000] {subprocess.py:93} INFO - 25/06/02 14:26:00 INFO CodeGenerator: Code generated in 316.336856 ms
[2025-06-02T14:26:00.313+0000] {subprocess.py:93} INFO - 25/06/02 14:26:00 INFO CodeGenerator: Code generated in 274.233706 ms
[2025-06-02T14:26:02.472+0000] {subprocess.py:93} INFO - 25/06/02 14:26:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2025-06-02T14:26:03.872+0000] {subprocess.py:93} INFO - 25/06/02 14:26:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-02T14:26:03.873+0000] {subprocess.py:93} INFO - 25/06/02 14:26:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2025-06-02T14:26:03.873+0000] {subprocess.py:93} INFO - 25/06/02 14:26:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-02T14:26:03.874+0000] {subprocess.py:93} INFO - 25/06/02 14:26:03 INFO DAGScheduler: Missing parents: List()
[2025-06-02T14:26:04.371+0000] {subprocess.py:93} INFO - 25/06/02 14:26:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-02T14:26:05.116+0000] {subprocess.py:93} INFO - 25/06/02 14:26:05 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1748874363873,WrappedArray(org.apache.spark.scheduler.StageInfo@260e5fb7),{spark.sql.warehouse.dir=file:/app/spark-warehouse, spark.app.initial.file.urls=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.app.initial.jar.urls=spark://41026342dcb2:36457/jars/com.google.code.findbugs_jsr305-3.0.0.jar,spark://41026342dcb2:36457/jars/org.spark-project.spark_unused-1.0.0.jar,spark://41026342dcb2:36457/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,spark://41026342dcb2:36457/jars/org.apache.kafka_kafka-clients-2.8.1.jar,spark://41026342dcb2:36457/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,spark://41026342dcb2:36457/jars/org.lz4_lz4-java-1.8.0.jar,spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,spark://41026342dcb2:36457/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,spark://41026342dcb2:36457/jars/commons-logging_commons-logging-1.1.3.jar,spark://41026342dcb2:36457/jars/org.slf4j_slf4j-api-1.7.32.jar,spark://41026342dcb2:36457/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,spark://41026342dcb2:36457/jars/org.apache.commons_commons-pool2-2.11.1.jar, spark.executor.id=driver, spark.files=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.driver.host=41026342dcb2, spark.jars=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.submit.deployMode=client, spark.app.submitTime=1748873981209, spark.sql.execution.id=0, spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0, spark.master=local[*], spark.serializer.objectStreamReset=100, spark.repl.local.jars=file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.submit.pyFiles=/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar,/opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,/opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,/opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,/opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,/opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,/opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,/opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar, spark.app.startTime=1748873996315, spark.rdd.compress=True, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.app.id=local-1748874035782, spark.app.name=KafkaLogConsumerBatch, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.driver.port=36457}) by listener AppStatusListener took 1.235320347s.
[2025-06-02T14:26:06.286+0000] {subprocess.py:93} INFO - 25/06/02 14:26:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 262.5 KiB, free 366.0 MiB)
[2025-06-02T14:26:11.483+0000] {subprocess.py:93} INFO - 25/06/02 14:26:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 92.5 KiB, free 366.0 MiB)
[2025-06-02T14:26:11.522+0000] {subprocess.py:93} INFO - 25/06/02 14:26:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 41026342dcb2:46171 (size: 92.5 KiB, free: 366.2 MiB)
[2025-06-02T14:26:12.364+0000] {subprocess.py:93} INFO - 25/06/02 14:26:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
[2025-06-02T14:26:13.080+0000] {subprocess.py:93} INFO - 25/06/02 14:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-02T14:26:13.081+0000] {subprocess.py:93} INFO - 25/06/02 14:26:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-06-02T14:26:17.674+0000] {subprocess.py:93} INFO - 25/06/02 14:26:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (41026342dcb2, executor driver, partition 0, PROCESS_LOCAL, 4631 bytes) taskResourceAssignments Map()
[2025-06-02T14:26:50.919+0000] {subprocess.py:93} INFO - 25/06/02 14:26:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-06-02T14:26:51.116+0000] {job.py:229} INFO - Heartbeat recovered after 45.02 seconds
[2025-06-02T14:27:00.077+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO ConsumerConfig: ConsumerConfig values:
[2025-06-02T14:27:00.087+0000] {job.py:229} INFO - Heartbeat recovered after 47.33 seconds
[2025-06-02T14:27:00.088+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-06-02T14:27:00.181+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-06-02T14:27:00.182+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-06-02T14:27:00.183+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-06-02T14:27:00.184+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-06-02T14:27:00.185+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-06-02T14:27:00.186+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2
[2025-06-02T14:27:00.186+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-06-02T14:27:00.187+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-06-02T14:27:00.188+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-06-02T14:27:00.193+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-06-02T14:27:00.210+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-06-02T14:27:00.280+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-06-02T14:27:00.452+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-06-02T14:27:00.453+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-06-02T14:27:00.454+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor
[2025-06-02T14:27:00.454+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-06-02T14:27:00.455+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-06-02T14:27:00.455+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-06-02T14:27:00.456+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-06-02T14:27:00.457+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-06-02T14:27:00.457+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-06-02T14:27:00.458+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T14:27:00.458+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-06-02T14:27:00.458+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-06-02T14:27:00.459+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-06-02T14:27:00.459+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-06-02T14:27:00.460+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-06-02T14:27:00.460+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-06-02T14:27:00.460+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-06-02T14:27:00.461+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-06-02T14:27:00.461+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[2025-06-02T14:27:00.462+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-06-02T14:27:00.462+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-06-02T14:27:00.462+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-06-02T14:27:00.463+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-06-02T14:27:00.463+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-06-02T14:27:00.463+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-06-02T14:27:00.464+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-06-02T14:27:00.464+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-06-02T14:27:00.464+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-06-02T14:27:00.464+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-06-02T14:27:00.465+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-06-02T14:27:00.465+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-06-02T14:27:00.465+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-06-02T14:27:00.466+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-06-02T14:27:00.466+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-06-02T14:27:00.466+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-06-02T14:27:00.466+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-06-02T14:27:00.467+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-06-02T14:27:00.467+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-06-02T14:27:00.467+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-06-02T14:27:00.467+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-06-02T14:27:00.468+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-06-02T14:27:00.468+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 10000
[2025-06-02T14:27:00.468+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-06-02T14:27:00.469+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-06-02T14:27:00.469+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-06-02T14:27:00.469+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2]
[2025-06-02T14:27:00.469+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-06-02T14:27:00.470+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-06-02T14:27:00.470+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-06-02T14:27:00.470+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-06-02T14:27:00.471+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-06-02T14:27:00.471+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-06-02T14:27:00.471+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-06-02T14:27:00.471+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-06-02T14:27:00.472+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-06-02T14:27:00.472+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.2
[2025-06-02T14:27:00.472+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-06-02T14:27:00.473+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-06-02T14:27:00.473+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-06-02T14:27:00.473+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-06-02T14:27:00.473+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-06-02T14:27:00.474+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-06-02T14:27:00.474+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-06-02T14:27:00.474+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-06-02T14:27:00.475+0000] {subprocess.py:93} INFO - 
[2025-06-02T14:27:00.475+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO AppInfoParser: Kafka version: 2.8.1
[2025-06-02T14:27:00.475+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2025-06-02T14:27:00.475+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO AppInfoParser: Kafka startTimeMs: 1748874420074
[2025-06-02T14:27:00.476+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Subscribed to partition(s): logs-0
[2025-06-02T14:27:00.476+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Seeking to EARLIEST offset of partition logs-0
[2025-06-02T14:27:00.476+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Cluster ID: KseVdgT7Th2WPRTGnxOBbw
[2025-06-02T14:27:00.477+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-06-02T14:27:00.477+0000] {subprocess.py:93} INFO - 25/06/02 14:27:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Seeking to LATEST offset of partition logs-0
[2025-06-02T14:27:02.054+0000] {subprocess.py:93} INFO - 25/06/02 14:27:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=190, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-06-02T14:27:03.493+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO CodeGenerator: Code generated in 1399.047024 ms
[2025-06-02T14:27:03.731+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO CodeGenerator: Code generated in 10.855659 ms
[2025-06-02T14:27:03.775+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO CodeGenerator: Code generated in 8.817154 ms
[2025-06-02T14:27:03.779+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-02T14:27:03.780+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-02T14:27:03.781+0000] {subprocess.py:93} INFO - 25/06/02 14:27:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2025-06-02T14:27:07.395+0000] {subprocess.py:93} INFO - 25/06/02 14:27:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Seeking to offset 0 for partition logs-0
[2025-06-02T14:27:07.776+0000] {subprocess.py:93} INFO - 25/06/02 14:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Seeking to EARLIEST offset of partition logs-0
[2025-06-02T14:27:08.300+0000] {subprocess.py:93} INFO - 25/06/02 14:27:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-06-02T14:27:08.300+0000] {subprocess.py:93} INFO - 25/06/02 14:27:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Seeking to LATEST offset of partition logs-0
[2025-06-02T14:27:08.301+0000] {subprocess.py:93} INFO - 25/06/02 14:27:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2, groupId=spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=190, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-06-02T14:27:09.187+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO CodeGenerator: Code generated in 321.931992 ms
[2025-06-02T14:27:09.634+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO FileOutputCommitter: Saved output of task 'attempt_202506021426018203086101373473264_0000_m_000000_0' to file:/app/output/error_logs/_temporary/0/task_202506021426018203086101373473264_0000_m_000000
[2025-06-02T14:27:09.635+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO SparkHadoopMapRedUtil: attempt_202506021426018203086101373473264_0000_m_000000_0: Committed. Elapsed time: 117 ms.
[2025-06-02T14:27:09.655+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2747 bytes result sent to driver
[2025-06-02T14:27:09.664+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 52691 ms on 41026342dcb2 (executor driver) (1/1)
[2025-06-02T14:27:09.666+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-06-02T14:27:09.673+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 64.853 s
[2025-06-02T14:27:09.677+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-02T14:27:09.677+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-06-02T14:27:09.680+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 67.210982 s
[2025-06-02T14:27:09.684+0000] {subprocess.py:93} INFO - 25/06/02 14:27:09 INFO FileFormatWriter: Start to commit write Job fba1ed97-70a7-4d8b-8fee-e98442855d35.
[2025-06-02T14:27:12.408+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO FileFormatWriter: Write Job fba1ed97-70a7-4d8b-8fee-e98442855d35 committed. Elapsed time: 2724 ms.
[2025-06-02T14:27:12.414+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO FileFormatWriter: Finished processing stats for write job fba1ed97-70a7-4d8b-8fee-e98442855d35.
[2025-06-02T14:27:12.898+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO Metrics: Metrics scheduler closed
[2025-06-02T14:27:12.899+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-06-02T14:27:12.900+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO Metrics: Metrics reporters closed
[2025-06-02T14:27:12.903+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-662cee85-6e0c-4f66-9f30-392844363549-executor-2 unregistered
[2025-06-02T14:27:12.904+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO SparkContext: Invoking stop() from shutdown hook
[2025-06-02T14:27:12.927+0000] {subprocess.py:93} INFO - 25/06/02 14:27:12 INFO SparkUI: Stopped Spark web UI at http://41026342dcb2:4040
[2025-06-02T14:27:13.007+0000] {subprocess.py:93} INFO - 25/06/02 14:27:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-02T14:27:14.407+0000] {subprocess.py:93} INFO - 25/06/02 14:27:14 INFO MemoryStore: MemoryStore cleared
[2025-06-02T14:27:14.495+0000] {subprocess.py:93} INFO - 25/06/02 14:27:14 INFO BlockManager: BlockManager stopped
[2025-06-02T14:27:14.495+0000] {subprocess.py:93} INFO - 25/06/02 14:27:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-02T14:27:14.496+0000] {subprocess.py:93} INFO - 25/06/02 14:27:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-02T14:27:15.080+0000] {subprocess.py:93} INFO - 25/06/02 14:27:15 INFO SparkContext: Successfully stopped SparkContext
[2025-06-02T14:27:15.081+0000] {subprocess.py:93} INFO - 25/06/02 14:27:15 INFO ShutdownHookManager: Shutdown hook called
[2025-06-02T14:27:15.081+0000] {subprocess.py:93} INFO - 25/06/02 14:27:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8
[2025-06-02T14:27:15.773+0000] {subprocess.py:93} INFO - 25/06/02 14:27:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-29f41363-71b1-4235-b167-e0f3ba678337
[2025-06-02T14:27:17.300+0000] {subprocess.py:93} INFO - 25/06/02 14:27:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-43faedc5-1992-4870-b285-ace4ee0febb8/pyspark-794f60c6-2578-46d0-adb8-5a007cfbc0ef
[2025-06-02T14:27:18.286+0000] {subprocess.py:93} INFO - Finished Spark Consumer
[2025-06-02T14:27:18.288+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-06-02T14:27:18.432+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-06-02T14:27:18.434+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=log_monitoring_dag, task_id=start_spark_consumer, run_id=manual__2025-06-02T14:13:28.002044+00:00, execution_date=20250602T141328, start_date=20250602T141831, end_date=20250602T142718
[2025-06-02T14:27:18.935+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-06-02T14:27:19.012+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-02T14:27:19.059+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
